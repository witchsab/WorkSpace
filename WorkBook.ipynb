{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bench mark \n",
    "import platform; print(platform.platform())\n",
    "import sys; print(\"Python\", sys.version)\n",
    "import numpy; print(\"NumPy\", numpy.__version__)\n",
    "import pandas; print(\"Pandas\", pandas.__version__)\n",
    "import scipy; print(\"SciPy\", scipy.__version__)\n",
    "import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\n",
    "import tensorflow as tf; print(\"Tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from imutils import paths\n",
    "from kneed import KneeLocator\n",
    "import pickle\n",
    "import Accuracy as accuracy\n",
    "import ImageSearch_Algo_Hash\n",
    "import ImageSearch_Algo_HSV\n",
    "import ImageSearch_Algo_ORB\n",
    "import ImageSearch_Algo_RGB\n",
    "import ImageSearch_Algo_SIFT\n",
    "import ImageSearch_Plots as myplots\n",
    "import Thresholding\n",
    "\n",
    "# # --------------- Reload modules on :\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "\n",
    "# --------------- TEST PARAMETERS ----------------------#\n",
    "# TESTNAME = \"Data519_RESIZE320\"\n",
    "# TESTNAME = \"DataUKBENCH10K\"\n",
    "TESTNAME = \"TestData519\"\n",
    "# TESTNAME = \"Data519\"\n",
    "\n",
    "# --------------- VAR COMMONS------------------\n",
    "\n",
    "# IMGDIR = r'./ukbench/'\n",
    "IMGDIR = r'./imagesbooks/'\n",
    "\n",
    "# IMGDIR = r'./images/imagesbooks_DENOISE2/'\n",
    "# IMGDIR = r'./images/imagesbooks_S160/'\n",
    "# IMGDIR = r'./images/imagesbooks_S320/'\n",
    "# IMGDIR = r'./images/imagesbooks_CT2.0/'\n",
    "# IMGDIR = r\"V:\\\\Download\\\\imagesbooks\\\\\"\n",
    "# IMGDIRPROCESSED = ['']*5\n",
    "# IMGDIRPROCESSED[0] = r\"V:\\\\Download\\\\imagesbooks1\\\\\"\n",
    "# IMGDIRPROCESSED[1] = r\"V:\\\\Download\\\\imagesbooks2\\\\\"\n",
    "# IMGDIRPROCESSED[2] = r\"V:\\\\Download\\\\imagesbooks3\\\\\"\n",
    "# IMGDIRPROCESSED[3] = r\"V:\\\\Download\\\\imagesbooks4\\\\\"\n",
    "# IMGDIRPROCESSED[4] = r\"V:\\\\Download\\\\imagesbooks_warp\\\\\"\n",
    "\n",
    "# --------------- CONFIG PARAMETERS ----------------------#\n",
    "\n",
    "ORB_FEATURES_LIMIT = 100\n",
    "ORB_N_CLUSTERS = 500\n",
    "SIFT_N_CLUSTERS = 500\n",
    "SIFT_FEATURES_LIMIT = 100\n",
    "LOWE_RATIO = 0.7\n",
    "SIFT_PREDICTIONS_COUNT = 100\n",
    "RGB_PARAMETERCORRELATIONTHRESHOLD = 0.70 # not needed for generation\n",
    "kneeHSV = 2\n",
    "kneeRGB = 2\n",
    "kneeORB = 2\n",
    "kneeSIFT = 2\n",
    "HASHLENGTH = 16\n",
    "\n",
    "# --------------- IMAGES  ----------------------#\n",
    "imagepaths =  (list(paths.list_images(IMGDIR)))\n",
    "myDataFiles = pd.DataFrame( {'file' : imagepaths })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install imagehash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- GENERATE ALL FEATURES & SAVE ------------ #\n",
    "\n",
    "# GEN SIFT\n",
    "sift_features_limit = SIFT_FEATURES_LIMIT\n",
    "lowe_ratio = LOWE_RATIO\n",
    "predictions_count = SIFT_PREDICTIONS_COUNT\n",
    "\n",
    "mydataSIFT, mytime1 = ImageSearch_Algo_SIFT.gen_sift_features(\n",
    "    imagepaths, sift_features_limit)\n",
    "print(\"SIFT Feature Generation time :\", mytime1)\n",
    "savefile = 'data/' + TESTNAME + '_PandasDF_SIFT_Features_kp'+ str(sift_features_limit)\n",
    "ImageSearch_Algo_SIFT.SIFT_SAVE_FEATURES (mydataSIFT, savefile)\n",
    "print(\"SIFT Feature saved to : \", savefile)\n",
    "# -- END\n",
    "\n",
    "# GEN ORB\n",
    "orb_features_limit = ORB_FEATURES_LIMIT\n",
    "\n",
    "mydataORB, mytime1 = ImageSearch_Algo_ORB.GEN_ORB_FEATURES(imagepaths, orb_features_limit)\n",
    "print(\"ORB Feature Generation time :\", mytime1)\n",
    "savefile = 'data/' + TESTNAME + '_PandasDF_ORB_Features_kp'+ str(orb_features_limit)\n",
    "ImageSearch_Algo_ORB.ORB_SAVE_FEATURES (mydataORB, savefile)\n",
    "print(\"ORB Feature saved to : \", savefile)\n",
    "# -- END\n",
    "\n",
    "# GEN RGB\n",
    "parametercorrelationthreshold = 0.70 # not needed for generation\n",
    "\n",
    "mydataRGB, mytime = ImageSearch_Algo_RGB.RGB_GEN(imagepaths)\n",
    "print('RGB Feature Generation time', mytime)\n",
    "savefile = 'data/' + TESTNAME + '_PandasDF_RGB_Features'\n",
    "ImageSearch_Algo_RGB.RGB_SAVE_FEATURES (mydataRGB, savefile)\n",
    "print(\"RGB Feature saved to : \", savefile)\n",
    "# -- END\n",
    "\n",
    "# GEN HSV\n",
    "mydataHSV, mytime = ImageSearch_Algo_HSV.HSV_GEN(imagepaths)\n",
    "print('HSV Feature Generation time', mytime)\n",
    "savefile = 'data/' + TESTNAME + '_PandasDF_HSV_Features'\n",
    "ImageSearch_Algo_HSV.HSV_SAVE_FEATURES (mydataHSV, savefile)\n",
    "print(\"HSV Feature saved to : \", savefile)\n",
    "# -- END\n",
    "\n",
    "\n",
    "# GEN HASH\n",
    "mydataHASH, mytime = ImageSearch_Algo_Hash.HASH_GEN(imagepaths, HASHLENGTH)\n",
    "print(\"HASH Features Generation time :\", mytime)\n",
    "savefile = 'data/' + TESTNAME + '_PandasDF_HASH_Features'\n",
    "ImageSearch_Algo_Hash.HASH_SAVE_FEATURES (mydataHASH, savefile)\n",
    "# -- END\n",
    "\n",
    "print (\"## Feature Generation Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------- GENERATE ALL TREES  ------------ #\n",
    "\n",
    "# RGB TREE\n",
    "savefile = 'data/' + TESTNAME + '_RGB_Tree'\n",
    "myRGBtree = ImageSearch_Algo_RGB.RGB_Create_Tree(mydataRGB, savefile=savefile)\n",
    "\n",
    "# HSV TREE\n",
    "savefile = 'data/' + TESTNAME + '_HSV_Tree'\n",
    "myHSVtree = ImageSearch_Algo_HSV.HSV_Create_Tree(mydataHSV, savefile=savefile)\n",
    "\n",
    "# HASH TREE\n",
    "AlgoGenList = ['whash', 'phash', 'dhash', 'ahash']\n",
    "for algo in AlgoGenList :\n",
    "    savefile = 'data/' + TESTNAME + '_HASH_Tree_' + str(algo)\n",
    "    myHASHTree = ImageSearch_Algo_Hash.HASH_Create_Tree(mydataHASH, savefile=savefile, hashAlgo=algo)\n",
    "\n",
    "# HASH TREE USE HYBRID HASH\n",
    "HybridAlgoList = ['whash', 'ahash']\n",
    "savefile = 'data/' + TESTNAME + '_HASH_Hybrid_Tree_' + str(('_').join (HybridAlgoList))\n",
    "myHybridtree = ImageSearch_Algo_Hash.HASH_CREATE_HYBRIDTREE(mydataHASH, savefile, HybridAlgoList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT FV Tree and Cluster\n",
    "n_clusters = SIFT_N_CLUSTERS\n",
    "savefile = 'data/' + TESTNAME + '_SIFT_Tree_Cluster' + str(n_clusters)\n",
    "mySIFTtree, mySIFTmodel, mySIFTFVHist = ImageSearch_Algo_SIFT.SIFT_CREATE_TREE_MODEL(mydataSIFT[:85], savefile, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataSIFT[:100]['siftdes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (mydataSIFT.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataSIFT.loc[877]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = mydataSIFT.loc[877]['siftdes']\n",
    "print (d)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mydataSIFT[mydataSIFT['siftdes'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mydataSIFT[mydataSIFT['siftdes']==[]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORB FV Tree and Cluster\n",
    "n_clusters = 500\n",
    "savefile = 'data/' + TESTNAME + '_ORB_Tree_Cluster' + str(n_clusters)\n",
    "myORBtree, myORBmodel, myORBFVHist = ImageSearch_Algo_ORB.ORB_CREATE_TREE_MODEL(mydataORB, savefile, n_clusters)\n",
    "\n",
    "print (\"## Tree Generation Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------  LOAD FEATUTES AND TREES from file  ------------ #\n",
    "\n",
    "HybridAlgoList = ['whash', 'ahash']\n",
    "AlgoGenList = ['whash', 'phash', 'dhash', 'ahash'] \n",
    "\n",
    "# Files \n",
    "file_HASH_Feature = 'data/' + TESTNAME + '_PandasDF_HASH_Features'\n",
    "file_HASH_HybridTree = 'data/' + TESTNAME + '_HASH_Hybrid_Tree_' + str(('_').join (HybridAlgoList))\n",
    "file_HSV_Cluster = 'data/' + 'test' + '_HSV_Cluster' + str(kneeHSV)\n",
    "file_HSV_Feature = 'data/' + TESTNAME + '_PandasDF_HSV_Features'\n",
    "file_HSV_Tree = 'data/' + TESTNAME + '_HSV_Tree'\n",
    "file_ORB_Cluster = 'data/' + 'test' + '_ORB_Cluster' + str(kneeORB)\n",
    "file_ORB_Feature = 'data/' + TESTNAME + '_PandasDF_ORB_Features_kp'+ str(ORB_FEATURES_LIMIT)\n",
    "file_ORB_TreeCluster = 'data/' + TESTNAME + '_ORB_Tree_Cluster' + str(ORB_N_CLUSTERS)\n",
    "file_Results = 'data/' + TESTNAME + '_Results'\n",
    "file_RGB_Cluster = 'data/' + 'test' + '_RGB_Cluster' + str(kneeRGB)\n",
    "file_RGB_Feature = 'data/' + TESTNAME + '_PandasDF_RGB_Features'\n",
    "file_RGB_Tree = 'data/' + TESTNAME + '_RGB_Tree'\n",
    "file_SIFT_Cluster = 'data/' + 'test' + '_SIFT_Cluster' + str(kneeSIFT)\n",
    "file_SIFT_Feature = 'data/' + TESTNAME + '_PandasDF_SIFT_Features_kp'+ str(SIFT_FEATURES_LIMIT)\n",
    "file_SIFT_TreeCluster = 'data/' + TESTNAME + '_SIFT_Tree_Cluster' + str(SIFT_N_CLUSTERS)\n",
    "\n",
    "# Features \n",
    "mydataRGB = ImageSearch_Algo_RGB.RGB_LOAD_FEATURES (file_RGB_Feature)\n",
    "mydataHSV = ImageSearch_Algo_HSV.HSV_LOAD_FEATURES (file_HSV_Feature)\n",
    "mydataSIFT = ImageSearch_Algo_SIFT.SIFT_LOAD_FEATURES (file_SIFT_Feature)\n",
    "mydataORB = ImageSearch_Algo_ORB.ORB_LOAD_FEATURES(file_ORB_Feature)\n",
    "mydataHASH = ImageSearch_Algo_Hash.HASH_LOAD_FEATURES(file_HASH_Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (mydataSIFT.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryimagepath = './ukbench/ukbench00926.jpg'\n",
    "queryimagepath = './ukbench/ukbench00925.jpg'\n",
    "\n",
    "queryimagepath = './ukbench/ukbench06528.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_img = cv2.imread(queryimagepath)    \n",
    "q_img = cv2.cvtColor(q_img, cv2.COLOR_BGR2RGB)\n",
    "sift = cv2.xfeatures2d.SIFT_create(100)\n",
    "sift = cv2.ORB_create(100)\n",
    "q_kp, q_des = sift.detectAndCompute(q_img, None)\n",
    "plt.imshow(cv2.drawKeypoints(q_img, q_kp, q_img.copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q_kp), len(q_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_des.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures = q_des.shape[1]\n",
    "nfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = numpy.zeros((2, nfeatures))\n",
    "padding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_kp = numpy.vstack((q_kp, padding))\n",
    "temp = q_des.astype('float')  # convert descriptors to float\n",
    "descriptors = temp[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataSIFT.loc[877]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = mydataSIFT.loc[877]['siftdes']\n",
    "print (d)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mydataSIFT.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving file  ./images/ukbench1/seed\n",
      "[INFO] Saving file  ./images/ukbench1/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench2/seed\n",
      "[INFO] Saving file  ./images/ukbench2/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_FEXT/seed\n",
      "[INFO] Saving file  ./images/ukbench_FEXT/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_FEXTwarp/seed\n",
      "[INFO] Saving file  ./images/ukbench_FEXTwarp/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_F_ALLBB/seed\n",
      "[INFO] Saving file  ./images/ukbench_F_ALLBB/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_S320/seed\n",
      "[INFO] Saving file  ./images/ukbench_S320/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_S160/seed\n",
      "[INFO] Saving file  ./images/ukbench_S160/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_EQ2/seed\n",
      "[INFO] Saving file  ./images/ukbench_EQ2/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_EQRGB/seed\n",
      "[INFO] Saving file  ./images/ukbench_EQRGB/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_CT2.0/seed\n",
      "[INFO] Saving file  ./images/ukbench_CT2.0/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_S32/seed\n",
      "[INFO] Saving file  ./images/ukbench_S32/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_R90/seed\n",
      "[INFO] Saving file  ./images/ukbench_R90/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_R180/seed\n",
      "[INFO] Saving file  ./images/ukbench_R180/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_R270/seed\n",
      "[INFO] Saving file  ./images/ukbench_R270/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_DENOISE1/seed\n",
      "[INFO] Saving file  ./images/ukbench_DENOISE1/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_DENOISE2/seed\n",
      "[INFO] Saving file  ./images/ukbench_DENOISE2/groundTruth\n",
      "[INFO] Saving file  ./images/ukbench_CTYING/seed\n",
      "[INFO] Saving file  ./images/ukbench_CTYING/groundTruth\n",
      "[INFO] Saving file  ./ukbench/seed\n",
      "[INFO] Saving file  ./ukbench/groundTruth\n"
     ]
    }
   ],
   "source": [
    "import AccuracyGlobal \n",
    "import pickle\n",
    "accuracy = AccuracyGlobal.AccuracyGlobal() # empty class genrated \n",
    "\n",
    "# --------------------  DEFINE DIRECTORIES ------------------- # \n",
    "# for hash all the images in folder / database \n",
    "\n",
    "IMGDIR              = r'./ukbench/'\n",
    "IMGDIRPROCESSED     = ['']*17\n",
    "\n",
    "IMGDIRPROCESSED[0]  = r'./images/ukbench1/'\n",
    "IMGDIRPROCESSED[1]  = r'./images/ukbench2/'\n",
    "IMGDIRPROCESSED[2]  = r'./images/ukbench_FEXT/'         # Warp in foreground BB 1\n",
    "IMGDIRPROCESSED[3]  = r'./images/ukbench_FEXTwarp/'     # Warp on foreground BB 2\n",
    "IMGDIRPROCESSED[4]  = r'./images/ukbench_F_ALLBB/'      # original img with overlayed BB \n",
    "IMGDIRPROCESSED[5]  = r'./images/ukbench_S320/'         # Resize to w=320 (50%)\n",
    "IMGDIRPROCESSED[6]  = r'./images/ukbench_S160/'         # Resize to w=160 (25%)\n",
    "IMGDIRPROCESSED[7]  = r'./images/ukbench_EQ2/'           # Equalized Histogram \n",
    "IMGDIRPROCESSED[8]  = r'./images/ukbench_EQRGB/'        # Equalized Histogram \n",
    "IMGDIRPROCESSED[9]  = r'./images/ukbench_CT2.0/'        # increased  contrast\n",
    "IMGDIRPROCESSED[10]  = r'./images/ukbench_S32/'         # Resize to w=32 (very small)\n",
    "IMGDIRPROCESSED[11]  = r'./images/ukbench_R90/'         # Rotate by 90 deg \n",
    "IMGDIRPROCESSED[12]  = r'./images/ukbench_R180/'         # Rotate by 180 deg \n",
    "IMGDIRPROCESSED[13]  = r'./images/ukbench_R270/'         # Rotate by 270 deg \n",
    "IMGDIRPROCESSED[14]  = r'./images/ukbench_DENOISE1/'         # Denoise  \n",
    "IMGDIRPROCESSED[15]  = r'./images/ukbench_DENOISE2/'         # Denoise  \n",
    "IMGDIRPROCESSED[16]  = r'./images/ukbench_CTYING/'        # increased  contrast\n",
    "\n",
    "def update_Preprocessed_Dicts() :\n",
    "    IMGDIRPROCESSED.append(IMGDIR) \n",
    "    for DIR in IMGDIRPROCESSED :\n",
    "        # DIR = IMGDIRPROCESSED[5] # test \n",
    "        thisDict = {}\n",
    "        haystackPaths = sorted(list(paths.list_images(DIR))) #[:2]\n",
    "        imagefiles = haystackPaths # [:50]\n",
    "        # print(haystackPaths)\n",
    "\n",
    "        for f in imagefiles: \n",
    "            gkey, gList = accuracy.accuracy_groundtruth_gen(f)\n",
    "            thisDict[gkey] = gList\n",
    "        # print (gList)\n",
    "        # store the file list \n",
    "        seedFile = DIR + 'seed'\n",
    "        outfile = open (seedFile + '.pickle', 'wb')\n",
    "        pickle.dump( thisDict.keys, outfile )\n",
    "        print (\"[INFO] Saving file \", seedFile)\n",
    "\n",
    "        # store the file matches (ground truth) dictionary \n",
    "        matchesFile = DIR + 'groundTruth'\n",
    "        outfile = open (matchesFile + '.pickle', 'wb')\n",
    "        pickle.dump( thisDict, outfile )\n",
    "        print (\"[INFO] Saving file \", matchesFile)\n",
    "\n",
    "# Run an updates on the preprocessed dicts \n",
    "update_Preprocessed_Dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Value 0\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print (accuracy.check_ground_truth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "from PIL import Image\n",
    "import time\n",
    "import foregroundextraction as extract\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to reload module: uncomment use the following \n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "\n",
    "# for hash all the images in folder / database \n",
    "\n",
    "IMGDIR = r\"../data/imagesbooks/\"\n",
    "IMGDIRPROCESSED = ['']*5\n",
    "IMGDIRPROCESSED[0] = r\"../data/imagesbooks1/\"\n",
    "IMGDIRPROCESSED[1] = r\"../data/imagesbooks2/\"\n",
    "IMGDIRPROCESSED[2] = r\"../data/imagesbooks3/\"\n",
    "IMGDIRPROCESSED[3] = r\"../data/imagesbooks4/\"\n",
    "IMGDIRPROCESSED[4] = r\"../data/imagesbooks_warp/\"\n",
    "\n",
    "\n",
    "# check directories, create if not exist\n",
    "for dir in IMGDIRPROCESSED : \n",
    "    if not os.path.exists(os.path.dirname(dir)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(dir))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haystackPaths = list(paths.list_images(IMGDIR)) #[:2]\n",
    "# print(haystackPaths)\n",
    "\n",
    "# test\n",
    "image1, image2, image3, image4, bbmap = extract.foregroundExtractAndWarp(haystackPaths[1])    # check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(bbmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run entire image set and create processed images\n",
    "\n",
    "# time preprocessing operation \n",
    "start = time.time()\n",
    "counter = 1 \n",
    "for f in haystackPaths:\n",
    "    \n",
    "    image_orig = Image.open(f)\n",
    "    # image1, image2 = extract.foregroundExtract(f)    # check \n",
    "    image1, image2, image3, image4, bbmap = extract.foregroundExtractAndWarp(f)    # check \n",
    "\n",
    "    img1 = Image.fromarray(image1)          # 0 filled bg                # check \n",
    "    img2 = Image.fromarray(image2)          # transparent bg             # check \n",
    "    img3 = Image.fromarray(image3)          # warped bounding box        # check \n",
    "    img4 = Image.fromarray(image4)          # warped inner bounding box  # check \n",
    "    img5 = Image.fromarray(bbmap)           # warped inner bounding box  # check \n",
    "    filename = os.path.basename(f).split('.')[0]\n",
    "    img1.save( IMGDIRPROCESSED[0] + filename + '.png', format='PNG')\n",
    "    img2.save( IMGDIRPROCESSED[1] + filename + '.png', format='PNG')\n",
    "    img3.save( IMGDIRPROCESSED[2] + filename + '.png', format='PNG')\n",
    "    img4.save( IMGDIRPROCESSED[3] + filename + '.png', format='PNG')\n",
    "    img5.save( IMGDIRPROCESSED[4] + filename + '.png', format='PNG')\n",
    "    print (\"Processed \" , counter , ' ', filename)\n",
    "    counter += 1\n",
    "\n",
    "print(\"[INFO] processed {} images in {:.2f} seconds\".format(\n",
    "len(haystackPaths), time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2Py3",
   "language": "python",
   "name": "venv2py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
